# 课程大纲

每一个板块，其中 1 小时讲解（理论）+ 1 小时实操（实践），总体 6h。

## 大模型微调

### 理论

1. 大模型简介与原理
2. 大模型构建过程
3. 微调与 Prompt Engineering、RAG 的区别与关系
4. 微调方法概览
    1. Full Fine-tuning
    2. Parameter-efficient Fine-tuning (PEFT) 高效参数微调
        1. LoRA（Low Rank Adaptation）
        2. Prefix Tuning / Prompt Tuning
    3. Instruction Tuning 指令微调
5. 微调流程解析
    - 数据准备（格式、清洗、对齐）
    - 预训练模型选择（LLM, embedding model, encoder-decoder）
    - 训练配置（超参数、硬件资源）
    - 评估与验证（loss、BLEU、ROUGE、人类评价）
6. 工具链与生态
    - HuggingFace / ModelScope
    - 微调框架：LLaMA-Factory / unsloth
    - 模型格式与部署（HF Hub, vLLM, Ollama）
7. 案例讲解
    - 举例：用 Llama / Qwen3 等开源模型做微调
    - 常见应用场景：文本分类、问答、对话、代码生成

### 实操

目标： 学员掌握一个微调流程的基本操作，能够独立跑通最小可用 Demo。

1. 环境准备
    - Python + PyTorch / CUDA
    - 安装 Hugging Face Transformers, Datasets, PEFT
    - 加载一个开源基础模型（如 Qwen/Qwen3-1.7B）
2. 数据准备
    - 准备一个小型任务数据集（例如：情感分类、FAQ 问答对）
    - 转换成 instruction-tuning 格式（JSON / JSONL）
3. LoRA 微调
    - 使用 PEFT 库加载模型
    - 配置 LoRA 参数（r, alpha, dropout）
    - 启动训练并实时查看 loss
    - 保存 adapter 权重
4. 推理与验证
    - 加载微调后的模型（基础模型 + LoRA 权重）
    - 验证微调模型回答效果
5. LLaMA-Factory 微调
    - 环境安装
    - 可视化微调

## MCP 开发课程大纲

### 第一部分：理论讲解（1 小时）

目标： 让学员理解 MCP 的背景、协议规范、应用场景和开发流程。

1. 背景
    - 什么是 MCP？
    - 为什么需要 MCP（传统 API 接口 vs MCP）
    - 原理：模型是如何确定工具的选用的？
2. MCP 核心概念
    - 核心架构：Hosts / Client / Servers
    - 传输层：Transport（Stdio transport/HTTP with SSE transport）
    - 核心概念：resources、tools、prompts、sampling 、roots
    - MCP Server vs MCP Client
3. MCP 应用场景
    - 将 AI 模型接入到 IDE / 应用
    - 与数据库、API、外部服务的交互
    - 构建插件生态（如 VS Code、浏览器插件、Agent 框架）
4. 开发流程概览
    - 搭建一个 MCP Server
    - 在 MCP Server 中注册资源/工具
    - 使用 MCP Client 调用
    - 如何在 IDE / 应用中消费 MCP 接口
5. 总结
    - MCP 的本质
    - MCP 的价值
    - 使用与开发

### 第二部分：实操演示（1 小时）

目标： 学员能够亲手实现一个最小可用的 MCP Server & Client Demo。

1. 环境准备
    - Node.js / Python 环境
    - 安装 MCP SDK（@modelcontextprotocol/sdk 或 Python 实现）
    - 准备调试工具（VS Code + 插件 / curl ）
2. 实现最小 MCP Server
    - 定义一个简单的 MCP Server（例如：天气查询 / 数据库查询 / 系统信息返回）
    - 注册一个 resource 和一个 tool
    - 启动 MCP Server 并监听
3. 实现一个 MCP Client
    - 使用 SDK 或手写 JSON-RPC 调用
    - 调用 server 暴露的资源和工具
    - 打印返回结果
4. 扩展与实验
    - 增加一个新工具（比如调用外部 API：翻译 / 股票价格）
    - 在客户端测试新增功能
    - （可选）在 IDE 插件里接入 MCP
5. 总结
    - MCP Server 的开发模式与常见场景
    - 下一步如何将 MCP 与 LLM 结合，做应用集成
    - 推荐资源与文档

## AI 大模型工作流开发

大模型 Agentic Workflows 开发课程大纲（2 小时）

### 第一部分：理论讲解（1 小时）

目标： 学员理解 Agentic Workflows 的背景、设计思想，以及目前业界主要解决方案。

1.  背景
    -   什么是 AI Agent?
    -   什么是 Agent 工作流？
    -   为什么需要 Agentic Workflows（单次 LLM 调用 → 智能体 → 复杂任务编排）
    -   应用场景：内容创作、业务流程自动化、客户服务 、数据分析等
2.  Agentic Workflows 核心要素
    -   大模型
    -   数据
    -   工具与技术
    -   流程与步骤
3.  AI 大模型工作流设计方法论

    -   输入规范化层：数据清洗、格式转换、敏感信息脱敏
    -   逻辑分解层：采用思维树（ToT）或思维链（CoT）进行任务拆解
    -   模型执行层：支持本地模型、云端 API、混合模式调度
    -   后处理层：结果排序、格式标准化、多模态融合
    -   反馈学习层：基于人工评分的数据回流优化

4.  典型模式
    -   链式工作流模式（Chain Workflow）
    -   并行化工作流模式（Parallelization Workflow）
    -   路由工作流模式（Routing Workflow）
5.  业界方案分享

    -   Dify
        -   特点：面向开发者的 LLMOps 平台，支持工作流可视化、RAG、Agent 编排
        -   优势：开箱即用、支持 API & UI 工作流构建
    -   扣子（Coze）
        -   特点：字节系出品，偏向对话式 Agent 构建
        -   优势：成熟商用平台

6.  实践前准备
    -   为什么选择 Dify 作为入门实践工具
    -   对比 LangChain 编程式方案，Dify 的优势在于 低门槛 + 可视化

### 第二部分：实操演示（1 小时）

目标： 学员亲手使用 Dify 搭建一个完整的 Agentic Workflow Demo。

1. 环境准备（docker/ollama）
    - 本地部署 Dify
    - 熟悉 Dify 工作流界面
2. 构建基础工作流
    - 创建一个新的工作流项目
    - 添加输入节点（用户问题）
    - 配置 LLM 节点（本地模型）
    - 测试基本问答流程
3. 增强为 RAG 工作流
    - 上传一个小型文档集（FAQ / 产品手册）
    - 配置向量数据库检索节点
    - 将检索结果传递给 LLM 节点
    - 演示：用户提问 → 检索 → 回答
4. 扩展 Agent 功能
    - 添加一个工具调用节点（例如：实时天气 API）
    - 演示 Agent 根据问题自动选择工具
    - 在 UI 中查看执行轨迹
5. 总结与拓展
    - 学员掌握了 Dify 工作流的基本开发流程
    - 对比其他方案（n8n、扣子）在实际业务中的应用

**最终收获**

-   学员理解 Agentic Workflows 的核心设计思想
-   能亲手在 Dify 上完成一个 RAG + 工具调用的最小工作流 Demo
