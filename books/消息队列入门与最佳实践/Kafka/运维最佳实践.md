# 运维最佳实践

## Lag监控

推荐使用我主导开源的项目[KCenter](https://github.com/xaecbd/KCenter)

如果希望自研代码监控Lag的话，推荐查看我写的文章：[如何监控kafka消费Lag情况](http://blog.trumandu.top/2019/04/13/%E5%A6%82%E4%BD%95%E7%9B%91%E6%8E%A7kafka%E6%B6%88%E8%B4%B9Lag%E6%83%85%E5%86%B5/)

## Rebalance Topic

Relalance Topic推荐采用官方脚本工具`kafka-reassign-partitions`和`kafka-preferred-replica-election`

工作原理是更改zk上的配置，然后触发选举生效。

**一、根据Topic 生成要迁移的partition方案**

```
./bin/kafka-reassign-partitions.sh --topics-to-move-json-file topics-to-move.json --broker-list "1,2,3,4,5,6,7" --generate --zookeeper localhost:2181
```
其中567为新加入的broker

topics-to-move.json
```
{"topics":
     [{"topic": "foo1"},{"topic": "foo2"}],
     "version":1
}
```
**二、执行partition方案**

```
$ ./bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file partitions-to-move.json --execute
```

partitions-to-move.json
```
{"partitions":
             [{"topic": "foo",
               "partition": 1,
               "replicas": [1,2,4] }],              
  "version":1
}
```

在执行的过程中记得保留原始分配方案

验证执行结果：
```
$ ./bin/kafka-reassign-partitions.sh --zookeeper localhost:2181  --reassignment-json-file partitions-to-move.json --verify
```

**三、优先副本重新选举**
```
bin/kafka-preferred-replica-election.sh --zookeeper localhost:12913/kafka --path-to-json-file topicPartitionList.json
```

topicPartitionList.json
```
{
 "partitions":
  [
    {"topic": "topic1", "partition": 0},
    {"topic": "topic1", "partition": 1},
    {"topic": "topic1", "partition": 2},
    {"topic": "topic2", "partition": 0},
    {"topic": "topic2", "partition": 1}
  ]
}
```

## 重新选举Kafka controller

在zookeeper上删除`/controller`

## 下线Kafka Leader

该操作可以参考relalance方式，唯一不同的是，不更改topic上partition分配方案，只是调整一下顺序。严格意义上不是下线，只是调整了leader。

**一、执行partition方案**

例如我们需要将topic:foo partiton 1 的原leader 2下线

原方案：

```
{"partitions":
             [{"topic": "foo",
               "partition": 1,
               "replicas": [2,1,4] }],              
  "version":1
}
```

只需要调整replicas数组的顺序就好。

```
$ ./bin/kafka-reassign-partitions.sh --reassignment-json-file partitions-to-move.json --execute
```

partitions-to-move.json
```
{"partitions":
             [{"topic": "foo",
               "partition": 1,
               "replicas": [1,4,2] }],              
  "version":1
}
```



验证执行结果：
```
$ ./bin/kafka-reassign-partitions.sh --reassignment-json-file partitions-to-move.json --verify
```

**二、优先副本重新选举**

```
bin/kafka-preferred-replica-election.sh --zookeeper localhost:12913/kafka --path-to-json-file topicPartitionList.json
```

topicPartitionList.json
```
{
 "partitions":
  [
    {"topic": "foo", "partition": 1},
  ]
}
```

## 机器更换

- 使用脚本(bin/kafka-server-stop.sh)停止broker
- 拷贝该机器数据目录到新机器上
- 新机器使用原机器的broker id,然后重新启动服务即可。

## 大流量应对之道

### 限流

### 数据删除

### 扩容

## 关键指标监控

- BytesIn/BytesOut：即Broker端每秒⼊站和出站字节数。你要确保这组值不要接近你的⽹络带宽，否则这通常都表示⽹卡已 被“打满”，很容易出现⽹络丢包的情形。
- MessagesIn:即Broker端每秒⼊站消息条数
- NetworkProcessorAvgIdlePercent：即⽹络线程池线程平均的空闲⽐例。通常来说，你应该确保这个JMX值⻓期⼤于 30%。如果⼩于这个值，就表明你的⽹络线程池⾮常繁忙，你需要通过增加⽹络线程数或将负载转移给其他服务器的⽅ 式，来给该Broker减负。
- RequestHandlerAvgIdlePercent：即I/O线程池线程平均的空闲⽐例。同样地，如果该值⻓期⼩于30%，你需要调整I/O线程 池的数量，或者减少Broker端的负载。
- UnderReplicatedPartitions：即未充分备份的分区数。所谓未充分备份，是指并⾮所有的Follower副本都和Leader副本保持 同步。⼀旦出现了这种情况，通常都表明该分区有可能会出现数据丢失。因此，这是⼀个⾮常重要的JMX指标。
- ISRShrink/ISRExpand：即ISR收缩和扩容的频次指标。如果你的环境中出现ISR中副本频繁进出的情形，那么这组值⼀定 是很⾼的。这时，你要诊断下副本频繁进出ISR的原因，并采取适当的措施。
- ActiveControllerCount：即当前处于激活状态的控制器的数量。正常情况下，Controller所在Broker上的这个JMX指标值应 该是1，其他Broker上的这个值是0。如果你发现存在多台Broker上该值都是1的情况，⼀定要赶快处理，处理⽅式主要是查 看⽹络连通性。这种情况通常表明集群出现了脑裂。脑裂问题是⾮常严重的分布式故障，Kafka⽬前依托ZooKeeper来防⽌ 脑裂。但⼀旦出现脑裂，Kafka是⽆法保证正常⼯作的。

## 参考
1. [PreferredReplicaLeaderElectionTool](https://cwiki.apache.org/confluence/display/KAFKA/Replication+tools#Replicationtools-1.PreferredReplicaLeaderElectionTool)
2. [replication](https://kafka.apache.org/documentation/#replication)